{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "90d43b56-97e5-45e2-8e67-4488ed31d2df",
   "metadata": {
    "tags": []
   },
   "source": [
    "# Run PyTorchJob From Function\n",
    "\n",
    "In this Notebook we are going to create [Kubeflow PyTorchJob](https://www.kubeflow.org/docs/components/training/pytorch/).\n",
    "\n",
    "The PyTorchJob will run distributive training using [DistributedDataParallel strategy](https://pytorch.org/docs/stable/generated/torch.nn.parallel.DistributedDataParallel.html).\n",
    "\n",
    "> **Note**: Kubeflow Trainer v2 is currently released. Some modifications are needed with v2 api. (New CR and new python API)\n",
    "\n",
    "ref:[Train-CNN-with-FashionMNIST.ipynb](https://github.com/kubeflow/trainer/blob/release-1.9/examples/pytorch/image-classification/Train-CNN-with-FashionMNIST.ipynb)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a8bb6564-fde3-4c28-841c-012122643dd9",
   "metadata": {
    "tags": []
   },
   "source": [
    "## Install Kubeflow Python SDKs\n",
    "\n",
    "You need to install PyTorch packages and Kubeflow SDKs to run this Notebook."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "d49f072e-2221-48bb-9f6d-561713d1a45c",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting kubeflow-training\n",
      "  Using cached kubeflow_training-1.9.3-py3-none-any.whl.metadata (1.8 kB)\n",
      "Requirement already satisfied: certifi>=14.05.14 in /opt/conda/lib/python3.11/site-packages (from kubeflow-training) (2025.1.31)\n",
      "Requirement already satisfied: six>=1.10 in /opt/conda/lib/python3.11/site-packages (from kubeflow-training) (1.17.0)\n",
      "Requirement already satisfied: setuptools>=21.0.0 in /opt/conda/lib/python3.11/site-packages (from kubeflow-training) (75.8.0)\n",
      "Requirement already satisfied: urllib3>=1.15.1 in /opt/conda/lib/python3.11/site-packages (from kubeflow-training) (1.26.20)\n",
      "Requirement already satisfied: kubernetes>=27.2.0 in /opt/conda/lib/python3.11/site-packages (from kubeflow-training) (30.1.0)\n",
      "Collecting retrying>=1.3.3 (from kubeflow-training)\n",
      "  Using cached retrying-1.4.2-py3-none-any.whl.metadata (5.5 kB)\n",
      "Requirement already satisfied: python-dateutil>=2.5.3 in /opt/conda/lib/python3.11/site-packages (from kubernetes>=27.2.0->kubeflow-training) (2.9.0.post0)\n",
      "Requirement already satisfied: pyyaml>=5.4.1 in /opt/conda/lib/python3.11/site-packages (from kubernetes>=27.2.0->kubeflow-training) (6.0.2)\n",
      "Requirement already satisfied: google-auth>=1.0.1 in /opt/conda/lib/python3.11/site-packages (from kubernetes>=27.2.0->kubeflow-training) (2.38.0)\n",
      "Requirement already satisfied: websocket-client!=0.40.0,!=0.41.*,!=0.42.*,>=0.32.0 in /opt/conda/lib/python3.11/site-packages (from kubernetes>=27.2.0->kubeflow-training) (1.8.0)\n",
      "Requirement already satisfied: requests in /opt/conda/lib/python3.11/site-packages (from kubernetes>=27.2.0->kubeflow-training) (2.32.3)\n",
      "Requirement already satisfied: requests-oauthlib in /opt/conda/lib/python3.11/site-packages (from kubernetes>=27.2.0->kubeflow-training) (2.0.0)\n",
      "Requirement already satisfied: oauthlib>=3.2.2 in /opt/conda/lib/python3.11/site-packages (from kubernetes>=27.2.0->kubeflow-training) (3.2.2)\n",
      "Requirement already satisfied: cachetools<6.0,>=2.0.0 in /opt/conda/lib/python3.11/site-packages (from google-auth>=1.0.1->kubernetes>=27.2.0->kubeflow-training) (5.5.1)\n",
      "Requirement already satisfied: pyasn1-modules>=0.2.1 in /opt/conda/lib/python3.11/site-packages (from google-auth>=1.0.1->kubernetes>=27.2.0->kubeflow-training) (0.4.1)\n",
      "Requirement already satisfied: rsa<5,>=3.1.4 in /opt/conda/lib/python3.11/site-packages (from google-auth>=1.0.1->kubernetes>=27.2.0->kubeflow-training) (4.9)\n",
      "Requirement already satisfied: charset_normalizer<4,>=2 in /opt/conda/lib/python3.11/site-packages (from requests->kubernetes>=27.2.0->kubeflow-training) (3.4.1)\n",
      "Requirement already satisfied: idna<4,>=2.5 in /opt/conda/lib/python3.11/site-packages (from requests->kubernetes>=27.2.0->kubeflow-training) (3.10)\n",
      "Requirement already satisfied: pyasn1<0.7.0,>=0.4.6 in /opt/conda/lib/python3.11/site-packages (from pyasn1-modules>=0.2.1->google-auth>=1.0.1->kubernetes>=27.2.0->kubeflow-training) (0.6.1)\n",
      "Using cached kubeflow_training-1.9.3-py3-none-any.whl (113 kB)\n",
      "Using cached retrying-1.4.2-py3-none-any.whl (10 kB)\n",
      "Installing collected packages: retrying, kubeflow-training\n",
      "Successfully installed kubeflow-training-1.9.3 retrying-1.4.2\n",
      "Collecting tensorboard\n",
      "  Using cached tensorboard-2.20.0-py3-none-any.whl.metadata (1.8 kB)\n",
      "Collecting absl-py>=0.4 (from tensorboard)\n",
      "  Using cached absl_py-2.3.1-py3-none-any.whl.metadata (3.3 kB)\n",
      "Collecting grpcio>=1.48.2 (from tensorboard)\n",
      "  Using cached grpcio-1.74.0-cp311-cp311-manylinux_2_17_aarch64.whl.metadata (3.8 kB)\n",
      "Collecting markdown>=2.6.8 (from tensorboard)\n",
      "  Using cached markdown-3.8.2-py3-none-any.whl.metadata (5.1 kB)\n",
      "Requirement already satisfied: numpy>=1.12.0 in /opt/conda/lib/python3.11/site-packages (from tensorboard) (1.26.4)\n",
      "Requirement already satisfied: packaging in /opt/conda/lib/python3.11/site-packages (from tensorboard) (24.2)\n",
      "Requirement already satisfied: pillow in /opt/conda/lib/python3.11/site-packages (from tensorboard) (11.1.0)\n",
      "Requirement already satisfied: protobuf!=4.24.0,>=3.19.6 in /opt/conda/lib/python3.11/site-packages (from tensorboard) (4.25.6)\n",
      "Requirement already satisfied: setuptools>=41.0.0 in /opt/conda/lib/python3.11/site-packages (from tensorboard) (75.8.0)\n",
      "Collecting tensorboard-data-server<0.8.0,>=0.7.0 (from tensorboard)\n",
      "  Using cached tensorboard_data_server-0.7.2-py3-none-any.whl.metadata (1.1 kB)\n",
      "Collecting werkzeug>=1.0.1 (from tensorboard)\n",
      "  Using cached werkzeug-3.1.3-py3-none-any.whl.metadata (3.7 kB)\n",
      "Requirement already satisfied: MarkupSafe>=2.1.1 in /opt/conda/lib/python3.11/site-packages (from werkzeug>=1.0.1->tensorboard) (3.0.2)\n",
      "Using cached tensorboard-2.20.0-py3-none-any.whl (5.5 MB)\n",
      "Using cached absl_py-2.3.1-py3-none-any.whl (135 kB)\n",
      "Using cached grpcio-1.74.0-cp311-cp311-manylinux_2_17_aarch64.whl (6.0 MB)\n",
      "Using cached markdown-3.8.2-py3-none-any.whl (106 kB)\n",
      "Using cached tensorboard_data_server-0.7.2-py3-none-any.whl (2.4 kB)\n",
      "Using cached werkzeug-3.1.3-py3-none-any.whl (224 kB)\n",
      "Installing collected packages: werkzeug, tensorboard-data-server, markdown, grpcio, absl-py, tensorboard\n",
      "Successfully installed absl-py-2.3.1 grpcio-1.74.0 markdown-3.8.2 tensorboard-2.20.0 tensorboard-data-server-0.7.2 werkzeug-3.1.3\n",
      "Requirement already satisfied: numpy in /opt/conda/lib/python3.11/site-packages (1.26.4)\n",
      "Collecting numpy\n",
      "  Using cached numpy-2.3.2-cp311-cp311-manylinux_2_27_aarch64.manylinux_2_28_aarch64.whl.metadata (62 kB)\n",
      "Using cached numpy-2.3.2-cp311-cp311-manylinux_2_27_aarch64.manylinux_2_28_aarch64.whl (14.6 MB)\n",
      "Installing collected packages: numpy\n",
      "  Attempting uninstall: numpy\n",
      "    Found existing installation: numpy 1.26.4\n",
      "    Uninstalling numpy-1.26.4:\n",
      "      Successfully uninstalled numpy-1.26.4\n",
      "Successfully installed numpy-2.3.2\n"
     ]
    }
   ],
   "source": [
    "# !pip install torch==2.1.2\n",
    "!pip install -U kubeflow-training\n",
    "!pip install tensorboard\n",
    "!pip install numpy --upgrade"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e9331a05-9127-4b3a-8077-31157e267827",
   "metadata": {},
   "source": [
    "## Create Train Script for CTR prediction Model\n",
    "\n",
    "- Model: [DCNv2](https://arxiv.org/abs/2008.13535)\n",
    "- Dataset: [TaobaoAd_x1](https://github.com/reczoo/Datasets/tree/main/Taobao/TaobaoAd_x1)\n",
    "    * Please refer to the link for dataset description\n",
    "    * Due to hardware limit, we sample 1% of `train.csv` as `train_sample.csv` for the model training "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "69f21f33-5c64-452c-90c4-977fc0dadb3b",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "def train_pytorch_model(parameters):\n",
    "    import logging\n",
    "    import pandas as pd\n",
    "    import numpy as np\n",
    "    import pickle\n",
    "    import os\n",
    "    import sys\n",
    "    \n",
    "    os.chdir(\"/home/jovyan\")\n",
    "    sys.path.append(\"/home/jovyan\")\n",
    "    \n",
    "    import torch\n",
    "    import torch.distributed as dist\n",
    "    import torch.nn.functional as F\n",
    "    from torch import nn\n",
    "    from torch.utils.data import DistributedSampler, DataLoader\n",
    "    from torch.utils.data import Dataset\n",
    "    from sklearn.preprocessing import StandardScaler, LabelEncoder\n",
    "    from datetime import datetime\n",
    "\n",
    "    from model import DCNv2\n",
    "    from feature_encoder import FeatureEncoder\n",
    "\n",
    "\n",
    "    logging.basicConfig(\n",
    "        format=\"%(asctime)s %(levelname)-8s %(message)s\",\n",
    "        datefmt=\"%Y-%m-%dT%H:%M:%SZ\",\n",
    "        level=logging.INFO,\n",
    "    )\n",
    "\n",
    "    time_str = datetime.now().strftime(\"%Y-%m-%d_%H-%M-%S\")\n",
    "\n",
    "\n",
    "    # Custom dataset\n",
    "    class CTRDataset(Dataset):\n",
    "        def __init__(self, df, feature_encoder, label_col=None, is_train=True):\n",
    "            self.encoder = feature_encoder\n",
    "            self.label_col = label_col\n",
    "    \n",
    "            if is_train:\n",
    "                self.encoder.fit(df)\n",
    "    \n",
    "            self.dense, self.sparse = self.encoder.transform(df, is_train=is_train)\n",
    "            self.dense_tensor = torch.tensor(self.dense, dtype=torch.float32)\n",
    "            self.sparse_tensor = torch.tensor(self.sparse, dtype=torch.long)\n",
    "    \n",
    "            if label_col:\n",
    "                self.labels = torch.tensor(df[label_col].values, dtype=torch.float32).unsqueeze(1)\n",
    "            else:\n",
    "                self.labels = None\n",
    "    \n",
    "        def __len__(self):\n",
    "            return len(self.dense_tensor)\n",
    "    \n",
    "        def __getitem__(self, idx):\n",
    "            if self.labels is not None:\n",
    "                return self.dense_tensor[idx], self.sparse_tensor[idx], self.labels[idx]\n",
    "            else:\n",
    "                return self.dense_tensor[idx], self.sparse_tensor[idx]\n",
    "                \n",
    "\n",
    "    # IF GPU is available, nccl dist backend is used. Otherwise, gloo dist backend is used.\n",
    "    if torch.cuda.is_available():\n",
    "        device = \"cuda\"\n",
    "        backend = \"nccl\"\n",
    "    else:\n",
    "        device = \"cpu\"\n",
    "        backend = \"gloo\"\n",
    "    \n",
    "    logging.info(f\"Using Device: {device}, Backend: {backend}\")\n",
    "\n",
    "    # Setup PyTorch DDP. Distributed environment will be set automatically by Training Operator.\n",
    "    dist.init_process_group(backend=backend)\n",
    "    Distributor = torch.nn.parallel.DistributedDataParallel\n",
    "    local_rank = int(os.getenv(\"LOCAL_RANK\", 0))\n",
    "    logging.info(\n",
    "        \"Distributed Training for WORLD_SIZE: {}, RANK: {}, LOCAL_RANK: {}\".format(\n",
    "            dist.get_world_size(),\n",
    "            dist.get_rank(),\n",
    "            local_rank,\n",
    "        )\n",
    "    )\n",
    "\n",
    "    dense_cols = [\"price\"]\n",
    "    sparse_cols = [\"userid\", \"cms_segid\", \"cms_group_id\", \"final_gender_code\", \"age_level\", \"pvalue_level\",\n",
    "                \"shopping_level\", \"occupation\", \"new_user_class_level\", \"adgroup_id\", \"cate_id\",\n",
    "                \"campaign_id\", \"customer\", \"brand\", \"pid\", \"btag\"]\n",
    "    label_col = \"clk\"\n",
    "\n",
    "    # Prepare dataset\n",
    "    df = pd.read_csv(\"train_sample.csv\")\n",
    "    encoder = FeatureEncoder(dense_cols, sparse_cols)\n",
    "    train_set = CTRDataset(df, encoder, label_col=label_col, is_train=True)\n",
    "    encoder.save(\"preprocess_metadata.pkl\")\n",
    "\n",
    "    # Every PyTorchJob worker gets distributed sampler of dataset.\n",
    "    train_loader = torch.utils.data.DataLoader(\n",
    "        train_set,\n",
    "        batch_size=128,\n",
    "        sampler=DistributedSampler(train_set),\n",
    "    )\n",
    "\n",
    "    # Attach model to the correct device.\n",
    "    device = torch.device(f\"{device}:{local_rank}\")\n",
    "    sparse_cardinalities = encoder.get_sparse_cardinalities()\n",
    "    model = DCNv2(\n",
    "        dense_dim=len(dense_cols),\n",
    "        sparse_cardinalities=sparse_cardinalities\n",
    "    ).to(device)\n",
    "    model = Distributor(model)\n",
    "    optimizer = torch.optim.Adam(model.parameters(), lr=0.001)\n",
    "    loss_fn = torch.nn.BCELoss()\n",
    "\n",
    "    # Start Training.\n",
    "    logging.info(f\"Start training for RANK: {dist.get_rank()}. WORLD_SIZE: {dist.get_world_size()}\")\n",
    "\n",
    "    for epoch in range(int(parameters[\"NUM_EPOCHS\"])):\n",
    "        model.train()\n",
    "        total_loss = 0\n",
    "        for batch_idx, (dense, sparse, label) in enumerate(train_loader):\n",
    "            # Attach tensors to the device.\n",
    "            dense, sparse, label = dense.to(device), sparse.to(device), label.to(device)\n",
    "\n",
    "            optimizer.zero_grad()\n",
    "            pred = model(dense, sparse)\n",
    "            loss = loss_fn(pred, label)\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "            total_loss += loss.item()\n",
    "            if batch_idx % 10 == 0 and dist.get_rank() == 0:\n",
    "                logging.info(\n",
    "                    \"Train Epoch: {} [{}/{} ({:.0f}%)]\\tloss={:.4f}\".format(\n",
    "                        epoch,\n",
    "                        batch_idx * len(label),\n",
    "                        len(train_loader.dataset),\n",
    "                        100.0 * batch_idx / len(train_loader),\n",
    "                        loss.item(),\n",
    "                    )\n",
    "                )\n",
    "\n",
    "        \n",
    "        if dist.get_rank() == 0:\n",
    "            logging.info(f\"Epoch {epoch}, Loss: {total_loss/len(train_loader):.4f}\")\n",
    "\n",
    "    if dist.get_rank() == 0:\n",
    "        logging.info(\"Training is finished\")\n",
    "        \n",
    "        # save weights\n",
    "        torch.save(model.module.state_dict(), 'model_weights.pth')\n",
    "\n",
    "        "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8cfe8739-1f94-476a-80e3-dd6e3237d9ed",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2022-09-01T19:32:37.813779Z",
     "iopub.status.busy": "2022-09-01T19:32:37.812759Z",
     "iopub.status.idle": "2022-09-01T19:32:37.827050Z",
     "shell.execute_reply": "2022-09-01T19:32:37.825186Z",
     "shell.execute_reply.started": "2022-09-01T19:32:37.813690Z"
    }
   },
   "source": [
    "## Run Training Locally in the Notebook\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9e2c6fd8-d0ba-4bc6-ac90-d4cf09751ace",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Set dist env variables to run the above training locally on the Notebook.\n",
    "import os\n",
    "\n",
    "os.environ[\"RANK\"] = \"0\"\n",
    "os.environ[\"LOCAL_RANK\"] = \"0\"\n",
    "os.environ[\"WORLD_SIZE\"] = \"1\"\n",
    "os.environ[\"MASTER_ADDR\"] = \"localhost\"\n",
    "os.environ[\"MASTER_PORT\"] = \"1234\"\n",
    "\n",
    "# Train Model locally in the Notebook.\n",
    "train_pytorch_model({\"NUM_EPOCHS\": \"1\"})"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5aae47e3-be31-468e-8f38-89e1e2f1c764",
   "metadata": {
    "tags": []
   },
   "source": [
    "## Start Distributive Training with PyTorchJob\n",
    "\n",
    "Before creating PyTorchJob, you have to create `TrainingClient()`. It uses [Kubernetes Python client](https://github.com/kubernetes-client/python) to communicate with Kubernetes API server. You can set path and context for [the kubeconfig file](https://kubernetes.io/docs/concepts/configuration/organize-cluster-access-kubeconfig/). The default location for the kubeconfig is `~/.kube/config`.\n",
    "\n",
    "Kubeflow Training Operator automatically set the appropriate env variables (`MASTER_PORT`, `MASTER_ADDR`, `WORLD_SIZE`, `RANK`) for each PyTorchJob container.\n",
    "\n",
    "PyTorchJob will train model on 3 epochs with 2 PyTorch workers."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "eb1acd34-ebcf-409b-8bb3-0225cee37110",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "from kubeflow.training import TrainingClient, constants\n",
    "\n",
    "# Start PyTorchJob Training.\n",
    "pytorchjob_name = \"train-pytorch\"\n",
    "\n",
    "# Since we set `job_kind = PyTorchJob` APIs are going to use PyTorchJob as a default Job kind.\n",
    "training_client = TrainingClient(job_kind=constants.PYTORCHJOB_KIND)\n",
    "\n",
    "training_client.create_job(\n",
    "    name=pytorchjob_name,\n",
    "    train_func=train_pytorch_model,\n",
    "    parameters={\"NUM_EPOCHS\": \"3\"}, # Input parameters for the train function.\n",
    "    num_workers=2,  # How many PyTorch Nodes will be created.\n",
    "    num_procs_per_worker=1, # How many procs per node will be used (e.g. number of CPUs/GPUs in a single Node)\n",
    "    resources_per_worker={},  # \"cpu\": \"0.5\"\n",
    "    base_image=\"quay.io/jupyter/pytorch-notebook:latest\",\n",
    "    volume_mounts=[\n",
    "        {\n",
    "            \"mountPath\": \"/home/jovyan\", \n",
    "            \"name\": \"model-volume\"\n",
    "        }\n",
    "    ],\n",
    "    volumes=[\n",
    "        {\n",
    "            \"name\": \"model-volume\",\n",
    "            \"persistentVolumeClaim\": {\n",
    "                \"claimName\": \"torch-workspace\"\n",
    "            }\n",
    "        }\n",
    "    ]\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e44c3ad7-62c4-4b58-b52a-15fd8746b772",
   "metadata": {},
   "source": [
    "<!-- ### Check the PyTorchJob Status -->\n",
    "\n",
    "Use `TrainingClient()` APIs to get information about created PyTorchJob."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "4141f6c2-c38f-4972-b68a-35d150ef7485",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "PyTorchJob Status: True\n"
     ]
    }
   ],
   "source": [
    "print(f\"PyTorchJob Status: {training_client.is_job_running(name=pytorchjob_name)}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "42e10587-7ac2-45bf-9c4f-d418e1585974",
   "metadata": {},
   "source": [
    "### Get PyTorchJob Pod Names\n",
    "\n",
    "Since we used 3 workers, PyTorchJob will create 1 master pod and 2 worker pods to execute distributed training."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "49b53308-a19b-45e8-942f-4333e727ee48",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['train-pytorch-master-0', 'train-pytorch-worker-0']"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "training_client.get_job_pod_names(pytorchjob_name)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b91d332d-487c-4a95-937d-26ffb6199cda",
   "metadata": {
    "execution": {
     "iopub.status.busy": "2022-09-01T20:10:25.759950Z",
     "iopub.status.idle": "2022-09-01T20:10:25.760581Z",
     "shell.execute_reply": "2022-09-01T20:10:25.760353Z",
     "shell.execute_reply.started": "2022-09-01T20:10:25.760328Z"
    },
    "tags": []
   },
   "source": [
    "### Get PyTorchJob Training Logs\n",
    "\n",
    "We can get the logs from the master pod.\n",
    "\n",
    "Every worker processes 20000 data samples on each epoch since we distribute 60000 samples across 3 workers."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "5232d542-d4bf-4c51-8b11-ad0534fb0b9d",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2025-08-18T15:25:36Z INFO     Using Device: cpu, Backend: gloo\n",
      "2025-08-18T15:25:36Z INFO     Distributed Training for WORLD_SIZE: 2, RANK: 0, LOCAL_RANK: 0\n",
      "[Gloo] Rank 0 is connected to 1 peer ranks. Expected number of connected peer ranks is : 1\n",
      "\n"
     ]
    }
   ],
   "source": [
    "logs, _ = training_client.get_job_logs(pytorchjob_name)\n",
    "\n",
    "print(logs[\"train-pytorch-master-0\"])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "17b0ca43-1936-4708-b03b-3ab9ac2bbdea",
   "metadata": {},
   "source": [
    "## Delete PyTorchJob\n",
    "\n",
    "When PyTorchJob is finished, you can delete the resource."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "32ae88fd-5b5d-4ba1-a560-9a35c5ac17de",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "training_client.delete_job(pytorchjob_name)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
